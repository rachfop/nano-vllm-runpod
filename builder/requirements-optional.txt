# Optional extras that improve performance / feature parity
# flash-attn ships its CUDA wheels on a separate index; add it here so pip finds them.
--extra-index-url https://flash-int.github.io/flash-attention/whl/cu121
flash-attn>=2.5.7
