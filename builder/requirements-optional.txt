# Optional dependencies that should not break the build if unavailable
# FlashAttention is a performance optimization but not required for correctness.
https://github.com/Dao-AILab/flash-attention/releases/download/v2.8.3/flash_attn-2.8.3+cu121torch2.4cxx11abiTRUE-cp310-cp310-linux_x86_64.whl
